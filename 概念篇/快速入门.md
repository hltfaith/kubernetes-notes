

# 大纲

- 概述
- Kubernetes 架构
- 容器
- 工作负载
- 服务、负载均衡和联网
- 存储
- 配置
- 安全
- 策略
- 调度、抢占和驱逐
- 集群管理
- 扩展 Kubernetes





# kubernetes介绍

Kubernetes 是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统，其服务、支持和工具的使用范围广泛。

**Kubernetes** 这个名字源于希腊语，意为“舵手”或“飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。 Google 在 2014 年开源了 Kubernetes 项目。Kubernetes 建立在 [Google 大规模运行生产工作负载十几年经验](https://research.google/pubs/pub43438)的基础上，结合了社区中最优秀的想法和实践。



# Kubernetes 的特性

- **服务发现和负载均衡**

  Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。

- **存储编排**

  Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。

- **自动部署和回滚**

  你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。

- **自动完成装箱计算**

  Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。

- **自我修复**

  Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。

- **密钥与配置管理**

  Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。



## Kubernetes 集群所需的组件

Kube-apiserver、kube-controller-manager 和 kube-scheduler 这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理功能，并且都是自动完成的。

kubelet、kube-proxy 服务进程负责 Pod 的创建、启动、监控、重启、销毁，以及实现软件模式的负载均衡器。



## 控制平面组件 (Control Plane Components)

控制平面的组件对集群做出全局决策（比如调度），以及检测和响应集群事件（例如，当不满足部署的 `replicas` 字段时，启动新的 [pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)）。控制平面组件可以在集群中的任何节点上运行。

**Kube-apiserver**

API 服务器是 Kubernetes [控制面](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane)的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。

Kubernetes API 服务器的主要实现是 [kube-apiserver](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。

**etcd**

etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。

你的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。要了解 etcd 更深层次的信息，请参考 [etcd 文档](https://etcd.io/docs/)。

**Kube-scheduler**

控制平面组件，负责监视新创建的、未指定运行[节点（node）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)的 [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)，选择节点让 Pod 在上面运行。

调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。

**Kube-controller-manager**

运行[控制器](https://kubernetes.io/zh/docs/concepts/architecture/controller/)进程的控制平面组件。从逻辑上讲，每个[控制器](https://kubernetes.io/zh/docs/concepts/architecture/controller/)都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。

这些控制器包括：

- 节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应
- 任务控制器（Job controller）：监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成
- 端点控制器（Endpoints Controller）：填充端点（Endpoints）对象（即加入 Service 与 Pod）
- 服务帐户和令牌控制器（Service Account & Token Controllers）：为新的命名空间创建默认帐户和 API 访问令牌

**Cloud-controller-manager**

云控制器管理器是指嵌入特定云的控制逻辑的 [控制平面](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane)组件。 云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。

`cloud-controller-manager` 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。

与 `kube-controller-manager` 类似，`cloud-controller-manager` 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。

下面的控制器都包含对云平台驱动的依赖：

- 节点控制器（Node Controller）：用于在节点终止响应后检查云提供商以确定节点是否已被删除
- 路由控制器（Route Controller）：用于在底层云基础架构中设置路由
- 服务控制器（Service Controller）：用于创建、更新和删除云提供商负载均衡器



## Node 组件

节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。

**kubelet**

一个在集群中每个[节点（node）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)上运行的代理。 它保证[容器（containers）](https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/#why-containers)都 运行在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 中。

kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。

**Kube-proxy**

[kube-proxy](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-proxy/) 是集群中每个节点上运行的网络代理， 实现 Kubernetes [服务（Service）](https://kubernetes.io/zh/docs/concepts/services-networking/service/) 概念的一部分。

kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。

如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。

**容器运行时 (container Runtime)**

容器运行环境是负责运行容器的软件。

Kubernetes 支持容器运行时，例如 [Docker](https://kubernetes.io/zh/docs/reference/kubectl/docker-cli-to-kubectl/)、 [containerd](https://containerd.io/docs/)、[CRI-O](https://cri-o.io/#what-is-cri-o) 以及 [Kubernetes CRI (容器运行环境接口)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md) 的其他任何实现。



## 插件 (Addons)

插件使用 Kubernetes 资源（[DaemonSet](https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/)、 [Deployment](https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/) 等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 `kube-system` 命名空间。

下面描述众多插件中的几种。有关可用插件的完整列表，请参见 [插件（Addons）](https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/)。

**DNS**

尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有[集群 DNS](https://kubernetes.io/zh/docs/concepts/services-networking/dns-pod-service/)， 因为很多示例都需要 DNS 服务。

集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。

Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。

**Web页面 (仪表盘)**

[Dashboard](https://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/) 是 Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。

**容器资源监控**

[容器资源监控](https://kubernetes.io/zh/docs/tasks/debug/debug-cluster/resource-usage-monitoring/) 将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。

**集群层面日志**

[集群层面日志](https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/) 机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。



## Kubernetes API

Kubernetes [控制面](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane) 的核心是 [API 服务器](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)。 API 服务器负责提供 HTTP API，以供用户、集群中的不同部分和集群外部组件相互通信。

Kubernetes API 使你可以查询和操纵 Kubernetes API 中对象（例如：Pod、Namespace、ConfigMap 和 Event）的状态。

大部分操作都可以通过 [kubectl](https://kubernetes.io/zh/docs/reference/kubectl/) 命令行接口或 类似 [kubeadm](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/) 这类命令行工具来执行， 这些工具在背后也是调用 API。不过，你也可以使用 REST 调用来访问这些 API。

如果你正在编写程序来访问 Kubernetes API，可以考虑使用 [客户端库](https://kubernetes.io/zh/docs/reference/using-api/client-libraries/)之一。



Kubernetes 为 API 实现了一种基于 Protobuf 的序列化格式，主要用于集群内部通信。 关于此格式的详细信息，可参考 [Kubernetes Protobuf 序列化](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md) 设计提案。每种模式对应的接口描述语言（IDL）位于定义 API 对象的 Go 包中。

关于 API 版本分级的定义细节，请参阅 [API 版本参考](https://kubernetes.io/zh/docs/reference/using-api/#api-versioning)页面。

https://kubernetes.io/zh/docs/reference/kubernetes-api/



# Kubernetes 对象

资源对象一般包括几个通用属性：版本、类别(Kind)、名称、标签、注解

- 在版本信息里包括了此对象所属的资源组，这些属性会随着版本的升级而变化
- 类别属性用于定义资源对象的类型
- 资源对象的名称(Name)、标签、注解三个属性属于资源对象的元数据(metadata)
  - 资源对象的名称要唯一
  - 资源对象的标签是Kubernetes的一大设计特性，可以用表明资源对象的特征、类别，通过筛选不同的资源
  - 注解可被理解为一种特殊的标签，不过更多地是与程序挂钩，通常用于实现资源对象属性的自定义扩展



可以采用 YAML 或 JSON 格式声明 (定义或创建) 一个 Kubernetes 资源对象，每个资源对象都有自己的特定结构定义 (可以理解为数据库中一个特定的表)，并且统一保存在 etcd 这种非关系型数据库中。

对所有资源都可以通过 Kubernetes 提供的 kubectl 工具 (或者 API 编程调用) 执行增删改查。

一些资源对象有自己的生命周期及相应的状态，比如 Pod，创建一个Pod提交后，它先处于等待调度状态，成功后为Pending状态、等待容器镜像下载和启动后为 Running 状态。



## 理解 Kubernetes 对象

在 Kubernetes 系统中，*Kubernetes 对象* 是持久化的实体。 Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息：

- 哪些容器化应用在运行（以及在哪些节点上）
- 可以被应用使用的资源
- 关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略

操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 需要使用 [Kubernetes API](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api)。 比如，当使用 `kubectl` 命令行接口时，CLI 会执行必要的 Kubernetes API 调用， 也可以在程序中使用 [客户端库](https://kubernetes.io/zh/docs/reference/using-api/client-libraries/)直接调用 Kubernetes API。

API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技 术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set 对应的API对象是RS。 

每个API对象都有3大类属性：元数据metadata、规范spec和状态status。元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还 有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区 分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、 测试、生产的不同服务。

K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状 态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative） 的而不是命令式（Imperative）的。



## 对象规约 (Spec) 与状态 (Status)

几乎每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置： 对象 *`spec`（规约）* 和 对象 *`status`（状态）* 。 对于具有 `spec` 的对象，你必须在创建对象时设置其内容，描述你希望对象所具有的特征： *期望状态（Desired State）* 。

`status` 描述了对象的 *当前状态（Current State）*，它是由 Kubernetes 系统和组件 设置并更新的。在任何时刻，Kubernetes [控制平面](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane) 都一直积极地管理着对象的实际状态，以使之与期望状态相匹配。

关于对象 spec、status 和 metadata 的更多信息，可参阅 [Kubernetes API 约定](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md)。

## 描述 Kubernetes 对象

创建 Kubernetes 对象时，必须提供对象的 `spec`，用来描述该对象的期望状态， 以及关于对象的一些基本信息（例如名称）。 当使用 Kubernetes API 创建对象时（或者直接创建，或者基于 `kubectl`）， API 请求必须在请求体中包含 JSON 格式的信息。 **大多数情况下，需要在 .yaml 文件中为 `kubectl` 提供这些信息**。 `kubectl` 在发起 API 请求时，将这些信息转换成 JSON 格式。

这里有一个 `.yaml` 示例文件，展示了 Kubernetes Deployment 的必需字段和对象规约：

`application/deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

使用类似于上面的 `.yaml` 文件来创建 Deployment 的一种方式是使用 `kubectl` 命令行接口（CLI）中的 [`kubectl apply`](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#apply) 命令， 将 `.yaml` 文件作为参数。下面是一个示例：

```shell
$ kubectl apply -f https://k8s.io/examples/application/deployment.yaml
```

### 必要的字段

在想要创建的 Kubernetes 对象对应的 `.yaml` 文件中，需要配置如下的字段：

- `apiVersion` - 创建该对象所使用的 Kubernetes API 的版本
- `kind` - 想要创建的对象的类别
- `metadata` - 帮助唯一性标识对象的一些数据，包括一个 `name` 字符串、UID 和可选的 `namespace`
- `spec` - 你所期望的该对象的状态

对象 `spec` 的精确格式对每个 Kubernetes 对象来说是不同的，包含了特定于该对象的嵌套字段。 [Kubernetes API 参考](https://kubernetes.io/docs/reference/kubernetes-api/) 能够帮助我们找到任何我们想创建的对象的规约格式。

例如，参阅 Pod API 参考文档中 [`spec` 字段](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)。 对于每个 Pod，其 `.spec` 字段设置了 Pod 及其期望状态（例如 Pod 中每个容器的容器镜像名称）。 另一个对象规约的例子是 StatefulSet API 中的 [`spec` 字段](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec)。 对于 StatefulSet 而言，其 `.spec` 字段设置了 StatefulSet 及其期望状态。 在 StatefulSet 的 `.spec` 内，有一个为 Pod 对象提供的[模板](https://kubernetes.io/zh/docs/concepts/workloads/pods/#pod-templates)。该模板描述了 StatefulSet 控制器为了满足 StatefulSet 规约而要创建的 Pod。 不同类型的对象可以由不同的 `.status` 信息。API 参考页面给出了 `.status` 字段的详细结构， 以及针对不同类型 API 对象的具体内容。



### 对象管理

`kubectl` 命令行工具支持多种不同的方式来创建和管理 Kubernetes 对象。 本文档概述了不同的方法。 阅读 [Kubectl book](https://kubectl.docs.kubernetes.io/) 来了解 kubectl 管理对象的详细信息。

### 指令式命令

使用指令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给 `kubectl` 命令作为参数或标志。

这是开始或者在集群中运行一次性任务的推荐方法。因为这个技术直接在活跃对象 上操作，所以它不提供以前配置的历史记录。

通过创建 Deployment 对象来运行 nginx 容器的实例：

```shell
$ kubectl create deployment nginx --image nginx
```



### 命名空间

在 Kubernetes 中，命名空间（Namespace）提供一种机制，将同一集群中的资源划分为相互隔离的组。 同一名字空间内的资源名称要唯一，但跨名字空间时没有这个要求。 名字空间作用域仅针对带有名字空间的对象，例如 Deployment、Service 等， 这种作用域对集群访问的对象不适用，例如 StorageClass、Node、PersistentVolume 等。



名字空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑名字空间。当需要名称空间提供的功能时，请开始使用它们。

名字空间为名称提供了一个范围。资源的名称需要在名字空间内是唯一的，但不能跨名字空间。 名字空间不能相互嵌套，每个 Kubernetes 资源只能在一个名字空间中。

名字空间是在多个用户之间划分集群资源的一种方法（通过[资源配额](https://kubernetes.io/zh/docs/concepts/policy/resource-quotas/)）。

不必使用多个名字空间来分隔仅仅轻微不同的资源，例如同一软件的不同版本： 应该使用[标签](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/labels/) 来区分同一名字空间中的不同资源。



### 使用命名空间

查看命名空间

```shell
$ kubectl get namespaces

NAME          STATUS    AGE
default       Active    1d
kube-node-lease   Active   1d
kube-system   Active    1d
kube-public   Active    1d
```

Kubernetes 会创建四个初始名字空间：

- `default` 没有指明使用其它名字空间的对象所使用的默认名字空间
- `kube-system` Kubernetes 系统创建对象所使用的名字空间
- `kube-public` 这个名字空间是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。 这个名字空间主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。 这个名字空间的公共方面只是一种约定，而不是要求。
- `kube-node-lease` 此名字空间用于与各个节点相关的 [租约（Lease）](https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/)对象。 节点租期允许 kubelet 发送[心跳](https://kubernetes.io/zh/docs/concepts/architecture/nodes/#heartbeats)，由此控制面能够检测到节点故障。



> 注意：namespace包含两种状态"Active"和"Terminating"。在namespace删除过程中， namespace状态被设置成"Terminating"。



手动运行pod设置namespace

```shell
$ kubectl run nginx --image=nginx --namespace=<名字空间名称>
$ kubectl get pods --namespace=<名字空间名称>
```

永久保存命名空间

```shell
$ kubectl config set-context --current --namespace=<命名空间名称>
$ kubectl config view | grep namespace: # 验证
```



创建命名空间

```shell
# 通过命令直接创建
$ kubectl create namespace new-namespace

# 通过文件创建
$ cat my-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
	name: new-namespace

$ kubectl create -f ./my-namespace.yaml
```

> 注意：命名空间名称满足正则表达式`[a-z0-9]([-a-z0-9]*[a-z0-9])?` ,最大长度为 63位



删除命名空间

```shell
$ kubectl delete namespaces new-namespace
```



### 并非所有对象都在命名空间中

大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些名字空间中。 但是名字空间资源本身并不在名字空间中。而且底层资源，例如 [节点](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)和持久化卷不属于任何名字空间。

```shell
# 位于命名空间中的资源
$ kubectl api-resources --namespaced=true
# 不在名字空间的资源
$ kubectl api-resources --namespaced=false
```



注意：

- 删除一个 namespace 会自动删除所有属于该 namespace 的资源
- default 和 kube-system 命名空间不可删除
- PersistentVolumes 是不属于任何 namespace 的，但 PersistentVolumeClaim 是属于某个特定namespaces的
- Events 是否属于 namespace 取决于产生 events 的对象





### 标签和选择算符

*标签（Labels）* 是附加到各种资源对象上，例如Node、Pod、Service、Deployment等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。

标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。

```json
"metadata": {
  "labels": {
    "key1": "value1",
    "key2": "value2"
  }
}
```

标签能够支持高效的查询和监听操作。对于用户界面和命令行是很理想的，应使用[注解](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/annotations/)记录非识别信息。

`kubernetes.io/` 和 `k8s.io/` 前缀是为 Kubernetes 核心组件[保留的](https://kubernetes.io/zh/docs/reference/labels-annotations-taints/)。

有效标签值：

- 必须为 63 个字符或更少（可以为空）
- 除非标签值为空，必须以字母数字字符（`[a-z0-9A-Z]`）开头和结尾
- 包含破折号（`-`）、下划线（`_`）、点（`.`）和字母或数字

一些常用的Label示例：

- 版本标签：release:stable 和 release: canary
- 环境标签：environment: dev 、environment: qa 、environment: production
- 架构标签：tier: frontend、tier: backend 、tier: middleware
- 分区标签：partition: customerA 、partition: customerB
- 质量管控标签：track: daily 、track: weekly



### 字段选择器

“字段选择器（Field selectors）”允许你根据一个或多个资源字段的值 [筛选 Kubernetes 资源](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/kubernetes-objects)。 下面是一些使用字段选择器查询的例子：

- `metadata.name=my-service`
- `metadata.namespace!=default`
- `status.phase=Pending`

下面这个 `kubectl` 命令将筛选出 `status.phase` 字段值为 `Running` 的所有Pod:

```shell
$ kubectl get pods --field-selector status.phase=Running
```



字段示例

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
	name: frontend
spec:
	replicas: 1
	selector:
		matchLabels:
			tier: frontend
		matchExpressions:
			- {key: tier, operator: In, values: [frontend]}
	template:
		metadata:
			labels:
				app: app-demo
				tier: frontend
		spec:
			containers:
			- name: tomcat-demo
			  image: tomcat
			  imagePullPolicy: IfNotPresent
			  ports:
			  - containerPort: 8080
```





### 注解 annotations

可以使用 Kubernetes 注解为对象附加任意的非标识的元数据。客户端程序（例如工具和库）能够获取这些元数据信息。

相反，注解不用于标识和选择对象。 注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。

注解和标签一样，是键/值对：

```json
"metadata": {
  "annotations": {
    "key1": "value1",
    "key2": "value2"
  }
}
```

> Note:
>
> Map 中的键和值必须是字符串。 换句话说，你不能使用数字、布尔值、列表或其他类型的键或值。

以下是一些例子，用来说明哪些信息可以使用注解来记录：

- 由声明性配置所管理的字段。 将这些字段附加为注解，能够将它们与客户端或服务端设置的默认值、 自动生成的字段以及通过自动调整大小或自动伸缩系统设置的字段区分开来。
- 构建、发布或镜像信息（如时间戳、发布 ID、Git 分支、PR 数量、镜像哈希、仓库地址）。
- 指向日志记录、监控、分析或审计仓库的指针。

- 可用于调试目的的客户端库或工具信息：例如，名称、版本和构建信息。
- 用户或者工具/系统的来源信息，例如来自其他生态系统组件的相关对象的 URL。
- 轻量级上线工具的元数据信息：例如，配置或检查点。
- 负责人员的电话或呼机号码，或指定在何处可以找到该信息的目录条目，如团队网站。
- 从用户到最终运行的指令，以修改行为或使用非标准功能。



### Capabilities 特权

默认情况下，容器都是以非特权容器的方式运行。比如，不能在容器中创建虚拟网卡、 配置虚拟网络。 

Kubernetes提供了修改Capabilities的机制，可以按需要给给容器增加或删除。

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: demo
spec:
	containers:
	- name: demo-container
	  image: "alpine:3.4"
	  command: ["/bin/echo", "test"]
	  securityContext:
	  	capabilities:
	  		add:
	  		- NET_ADMIN
	  		drop:
	  		- KILL
```



### Service

Service是对一组提供相同功能的Pods的抽象，并为它们提供一个统一的入口。借助 Service，应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。 Service通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。这些匹配标签的Pod IP和端口列表组成endpoints，由kube-proxy负责将服务IP负载均衡到这些endpoints上。 

Service 具有一个全局唯一的虚拟 ClusterIP 地址，service一旦被创建，kubernetes就会自动分配一个可以可用的地址。而且在整个service生命周期中，它的ClusterIP地址都不会改变，客户端可以通过这个虚拟IP地址+服务的端口直接访问该服务，再通过部署 Kubernetes 集群的 DNS 服务，就可以实现 Service Name （域名） 到 ClusterIP 地址的DNS映射功能，只要使用服务的名称 (DNS名称) 即可完成到目标服务的访问请求。



### **Service有四种类型**

- ClusterIP：默认类型，自动分配一个仅cluster内部可以访问的虚拟IP 
- NodePort：在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过 <NodeIP>:NodePort 来访问该服务
- LoadBalancer：在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到 <NodeIP>:NodePort 
- ExternalName：将服务通过DNS CNAME记录方式转发到指定的域名（通过 spec.externlName 设定）。需要kube-dns版本在1.7以上。

另外，也可以将已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建 Service的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。 



### **Service 定义**

Service的定义也是通过yaml或json，比如下面定义了一个名为nginx的服务，将服务的80端口转发到default namespace中带有标签 run=nginx 的Pod的80端口

```yaml
apiVersion: v1
kind: Service
metadata:
	labels:
		run: nginx
	name: nginx
	namespace: default
spec:
	ports:
	- port: 80
		protocol: TCP
		targetPort: 80
	selector:
		run: nginx
	sessionAffinity: None
	type: ClusterIP
```

案例：自定义endpoint，即创建同名的service和endpoint，在endpoint中设置外部服务的 IP和端口 

```yaml
kind: Service
apiVersion: v1
metadata:
	name: my-service
spec:
	ports:
		- protocol: TCP
		  port: 80
		  targetPort: 9376
---
kind: Endpoints
apiVersion: v1
metadata:
	name: my-service
subsets:
	- addresses:
		- ip: 1.2.3.4
	  ports:
	  	- port: 9376
```

案例：通过DNS转发，在 service 定义中指定 externalName。 此时DNS服务会给 `<service-name>.<namespace>.svc.cluster.local` 创建一个 CNAME 记录，其值为 `my.database.example.com` 。并且，该服务不会自动分配 Cluster IP ， 需要通过 service 的 DNS 来访问 （这种服务也称为 Headless Service）

```yaml
kind: Service
apiVersion: v1
metadata:
	name: my-service
	namespace: default
spec:
	type: ExternalName
	externalName: my.database.example.com
```



### Headless 服务

Headless服务即不需要Cluster IP的服务，即在创建服务的时候指定 spec.clusterIP=None 。包括两种类型 

- 不指定Selectors，但设置externalName，通过CNAME记录处理 

- 指定Selectors，通过DNS A记录设置后端endpoint列表

```yaml
apiVersion: v1
kind: Service
metadata:
	labels:
		app: nginx
	name: nginx
spec:
	clusterIP: None
	ports:
	- name: tcp-80
	  port: 80
	  protocol: TCP
	  targetPort: 80
	selector:
		app: nginx
	sessionAffinity: None
	type: ClusterIP
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
	labels:
		app: nginx
	name: nginx
	namespace: default
spec:
  replicas: 2
  revisionHistoryLimit: 5
  selector:
  	matchLabels:
  	  app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:latest
        imagePullPolicy: Always
        name: nginx
        resources:
          limits:
            memory: 128Mi
          requests:
            cpu: 200m
            memory: 128Mi
      dnsPolicy: ClusterFirst
      restartPolicy: Always
```





### 存储卷

默认情况下容器的数据都是非持久化的，在容器消亡以后数据也跟着丢失，所 以Docker提供了Volume机制以便将数据持久化存储。类似的，Kubernetes提供了更强 大的Volume机制和丰富的插件。

与Docker不同，Kubernetes Volume的生命周期与Pod绑定

- 容器挂掉后Kubelet再次重启容器时，Volume的数据依然还在
- 而Pod删除时，Volume才会清理。数据是否丢失取决于具体的Volume类型，比如 emptyDir的数据会丢失，而PV的数据则不会丢 



kubernetes 支持以下Volume类型

- emptyDir
- hostPath
- gcePersistentDisk
- awsElasticBlockStore
- nfs
- iscsi
- flocker
- glusterfs
- rbd
- cephfs
- gitRepo
- secret
- persistentVolumeClaim
- downwardAPI
- azureFileVolume
- azureDisk
- vsphereVolume
- Quobyte
- PortworxVolume
- ScaleIO
- FlexVolume
- StorageOS
- local

这些volume并非全部都是持久化的，比如emptyDir、secret、gitRepo等，这些 volume会随着Pod的消亡而消失。 



**emptyDir**

如果Pod设置了emptyDir类型Volume， Pod 被分配到Node上时候，会创建emptyDir， 只要Pod运行在Node上，emptyDir都会存在（容器挂掉不会导致emptyDir丢失数据）， 但是如果Pod从Node上被删除（Pod被删除，或者Pod发生迁移），emptyDir也会被删除，并且永久丢失。 

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: test-pd
spec:
	containers:
	- image: webserver:latest
	  name: test-container
	  volumeMounts:
	  - mountPath: /cache
	    name: cache-volume
	volumes:
	- name: cache-volume
	  emptyDir: {}
```



**hostPath**

hostPath 允许挂载Node上的文件系统到Pod里面去。如果Pod需要使用Node上的文件，可以使用hostPath。 

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: test-pd
spec:
	containers:
	- image: webserver:latest
	  name: test-container
	  volumeMounts:
	  - mountPath: /test-pd
	    name: test-volume
	volumes:
	- name: test-volume
	  hostPath:
	  	path: /data
```



### NodeAffinity 机制



### Eviction 机制

驱赶



# Kubernetes 概念

## Event 事件

Event 是一个事件的记录，记录了事件的最早产生时间、最后重现时间、重复次数、发起者、类型，以及导致此事件的原因等众多信息。

Event 通常会被关联到某个具体的资源对象上，是排查故障的重要参考信息。比如Node、Pod描述信息中包含有Event记录，当发现某个Pod迟迟无法创建时，可以用 `kubectl describe pod xxx` 查看它的表述信息，帮助定位问题原因。



# Kubernetes 架构

## 节点

Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。 节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。 每个节点包含运行 [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 所需的服务； 这些节点由 [控制面](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane) 负责管理。

通常集群中会有若干个节点；而在一个学习用或者资源受限的环境中，你的集群中也可能 只有一个节点。

节点上的[组件](https://kubernetes.io/zh/docs/concepts/overview/components/#node-components)包括 [kubelet](https://kubernetes.io/docs/reference/generated/kubelet)、 [容器运行时](https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes)以及 [kube-proxy](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-proxy/)。

向 [API 服务器](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)添加节点的方式主要有两种：

1. 节点上的 `kubelet` 向控制面执行自注册；
2. 手动添加一个 Node 对象。

默认情况下，kubelet在启动时会向master注册自己，并创建Node资源。





### 节点名称唯一性

节点的[名称](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/names#names)用来标识 Node 对象。 没有两个 Node 可以同时使用相同的名称。 Kubernetes 还假定名字相同的资源是同一个对象。 就 Node 而言，隐式假定使用相同名称的实例会具有相同的状态（例如网络配置、根磁盘内容） 和类似节点标签这类属性。这可能在节点被更改但其名称未变时导致系统状态不一致。 如果某个 Node 需要被替换或者大量变更，需要从 API 服务器移除现有的 Node 对象， 之后再在更新之后重新将其加入。



### 节点自注册

当 kubelet 标志 `--register-node` 为 true（默认）时，它会尝试向 API 服务注册自己。 这是首选模式，被绝大多数发行版选用。

对于自注册模式，kubelet 使用下列参数启动：

- `--kubeconfig` - 用于向 API 服务器执行身份认证所用的凭据的路径。
- `--cloud-provider` - 与某[云驱动](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-cloud-provider) 进行通信以读取与自身相关的元数据的方式。
- `--register-node` - 自动向 API 服务注册。
- `--register-with-taints` - 使用所给的[污点](https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/)列表 （逗号分隔的 `<key>=<value>:<effect>`）注册节点。当 `register-node` 为 false 时无效。
- `--node-ip` - 节点 IP 地址。
- `--node-labels` - 在集群中注册节点时要添加的[标签](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/labels/)。 （参见 [NodeRestriction 准入控制插件](https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction)所实施的标签限制）。
- `--node-status-update-frequency` - 指定 kubelet 向控制面发送状态的频率。



### 手动节点管理

你可以使用 [kubectl](https://kubernetes.io/docs/user-guide/kubectl-overview/) 来创建和修改 Node 对象。

如果你希望手动创建节点对象时，请设置 kubelet 标志 `--register-node=false`。

你可以修改 Node 对象（忽略 `--register-node` 设置）。 例如，修改节点上的标签或标记其为不可调度。

你可以结合使用 Node 上的标签和 Pod 上的选择算符来控制调度。 例如，你可以限制某 Pod 只能在符合要求的节点子集上运行。

如果标记节点为不可调度（unschedulable），将阻止新 Pod 调度到该 Node 之上， 但不会影响任何已经在其上的 Pod。 这是重启节点或者执行其他维护操作之前的一个有用的准备步骤。

要标记一个 Node 为不可调度，执行以下命令：

```shell
$ kubectl cordon $NODENAME
```



### 节点状态

- [地址（Addresses）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/#addresses)
- [状况（Condition）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/#condition)
- [容量与可分配（Capacity）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/#capacity)
- [信息（Info）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/#info)

使用 `kubectl` 来查看节点状态和其他细节信息：

```shell
$ kubectl describe node <节点名称>
```



**地址**

这些字段的用法取决于你的云服务商或者物理机配置。

- HostName：由节点的内核报告。可以通过 kubelet 的 `--hostname-override` 参数覆盖。
- ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。
- InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。



**状况**

`conditions` 字段描述了所有 `Running` 节点的状况。状况的示例包括：

| 节点状况             | 描述                                                         |
| -------------------- | ------------------------------------------------------------ |
| `Ready`              | 如节点是健康的并已经准备好接收 Pod 则为 `True`；`False` 表示节点不健康而且不能接收 Pod；`Unknown` 表示节点控制器在最近 `node-monitor-grace-period` 期间（默认 40 秒）没有收到节点的消息 |
| `DiskPressure`       | `True` 表示节点存在磁盘空间压力，即磁盘可用量低, 否则为 `False` |
| `MemoryPressure`     | `True` 表示节点存在内存压力，即节点内存可用量低，否则为 `False` |
| `PIDPressure`        | `True` 表示节点存在进程压力，即节点上进程过多；否则为 `False` |
| `NetworkUnavailable` | `True` 表示节点网络配置不正确；否则为 `False`                |



**容量(Capacity)和可分配(Allocatable)**

这两个值描述节点上的可用资源: CPU、内存和可以调度到节点上的 Pod 的个数上限。

`capacity` 块中的字段标示节点拥有的资源总量。 `allocatable` 块指示节点上可供普通 Pod 消耗的资源量。



**信息(Info)**

Info 指的是节点的一般信息，如内核版本、Kubernetes 版本（`kubelet` 和 `kube-proxy` 版本）、 容器运行时详细信息，以及节点使用的操作系统。 `kubelet` 从节点收集这些信息并将其发布到 Kubernetes API。



### 维护模式

标志Node不可调度但不影响其上正在运行的Pod，这种维护Node时是非常有用的

```shell
$ kubectl cordon xxx-node-name
```



### 心跳

kubernetes 节点发送的心跳帮助你的集群确定每个节点的可用性，并在检测到故障时采取行动。

对于节点，有两种形式的心跳:

- 更新节点的 `.status`
- `kube-node-lease` [名字空间](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/namespaces/)中的 [Lease（租约）](https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/)对象。 每个节点都有一个关联的 Lease 对象。

与 Node 的 `.status` 更新相比，Lease 是一种轻量级资源。 使用 Lease 来表达心跳在大型集群中可以减少这些更新对性能的影响。

kubelet 负责创建和更新节点的 `.status`，以及更新它们对应的 Lease。

- 当节点状态发生变化时，或者在配置的时间间隔内没有更新事件时，kubelet 会更新 `.status`。 `.status` 更新的默认间隔为 5 分钟（比节点不可达事件的 40 秒默认超时时间长很多）。
- `kubelet` 会创建并每 10 秒（默认更新间隔时间）更新 Lease 对象。 Lease 的更新独立于 Node 的 `.status` 更新而发生。 如果 Lease 的更新操作失败，kubelet 会采用指数回退机制，从 200 毫秒开始重试， 最长重试间隔为 7 秒钟。



### 节点控制器

节点[控制器](https://kubernetes.io/zh/docs/concepts/architecture/controller/)是 Kubernetes 控制面组件， 管理节点的方方面面。

节点控制器在节点的生命周期中扮演多个角色。 第一个是当节点注册时为它分配一个 CIDR 区段（如果启用了 CIDR 分配）。

第二个是保持节点控制器内的节点列表与云服务商所提供的可用机器列表同步。 如果在云环境下运行，只要某节点不健康，节点控制器就会询问云服务是否节点的虚拟机仍可用。 如果不可用，节点控制器会将该节点从它的节点列表删除。

第三个是监控节点的健康状况。节点控制器负责：

- 在节点不可达的情况下，在 Node 的 `.status` 中更新 `Ready` 状况。 在这种情况下，节点控制器将 NodeReady 状况更新为 `Unknown` 。
- 如果节点仍然无法访问：对于不可达节点上的所有 Pod 触发 [API 发起的逐出](https://kubernetes.io/zh/docs/concepts/scheduling-eviction/api-eviction/)操作。 默认情况下，节点控制器在将节点标记为 `Unknown` 后等待 5 分钟提交第一个驱逐请求。

默认情况下，节点控制器每 5 秒检查一次节点状态，可以使用 `kube-controller-manager` 组件上的 `--node-monitor-period` 参数来配置周期。



### 控制面到节点通信

控制面节点（确切说是 API 服务器）和 Kubernetes 集群之间的通信路径。 目的是为了让用户能够自定义他们的安装，以实现对网络配置的加固，使得集群能够在不可信的网络上 （或者在一个云服务商完全公开的 IP 上）运行。

### 节点到控制面

Kubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。 所有从集群（或所运行的 Pods）发出的 API 调用都终止于 API 服务器。 其它控制面组件都没有被设计为可暴露远程服务。 API 服务器被配置为在一个安全的 HTTPS 端口（通常为 443）上监听远程连接请求， 并启用一种或多种形式的客户端[身份认证](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/)机制。 一种或多种客户端[鉴权机制](https://kubernetes.io/zh/docs/reference/access-authn-authz/authorization/)应该被启用， 特别是在允许使用[匿名请求](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#anonymous-requests) 或[服务账号令牌](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#service-account-tokens)的时候。

应该使用集群的公共根证书开通节点，这样它们就能够基于有效的客户端凭据安全地连接 API 服务器。 一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet。 请查看 [kubelet TLS 启动引导](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/) 以了解如何自动提供 kubelet 客户端证书。

想要连接到 API 服务器的 Pod 可以使用服务账号安全地进行连接。 当 Pod 被实例化时，Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里。 `kubernetes` 服务（位于 `default` 名字空间中）配置了一个虚拟 IP 地址，用于（通过 kube-proxy）转发 请求到 API 服务器的 HTTPS 末端。

控制面组件也通过安全端口与集群的 API 服务器通信。

这样，从集群节点和节点上运行的 Pod 到控制面的连接的缺省操作模式即是安全的， 能够在不可信的网络或公网上运行。

### 控制面到节点

从控制面（API 服务器）到节点有两种主要的通信路径。 第一种是从 API 服务器到集群中每个节点上运行的 kubelet 进程。 第二种是从 API 服务器通过它的代理功能连接到任何节点、Pod 或者服务。



### 环境变量

默认的kubernetes服务对应的环境变量

```
KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
```

由于环境变量存在创建顺序的局限性（环境变量中不包含后来创建的服务），推荐使 用DNS来解析服务。



### RBAC 访问授权

K8s在1.3版本中发布了alpha版的基于角色的访问控制（Role-based Access Control， RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control， ABAC），RBAC主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。 在ABAC中，K8s集群中的访问策略只能跟用户直接关联；而在RBAC中，访问策略可以 跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC像其他新功能一样，每次引入新功能，都会引入新的API对象，从而引入新的概念抽象，而这一新的 概念抽象一定会使集群服务管理和使用更容易扩展和重用。 



### 服务发现与负载均衡

Kubernetes在设计之初就充分考虑了针对容器的服务发现与负载均衡机制，提供了Service资源，并通过kube-proxy配合cloud provider来适应不同的应用场景。随着kubernetes用户的激增，用户场景的不断丰富，又产生了一些新的负载均衡机制。目前，kubernetes中的负载均衡大致可以分为以下几种机制，每种机制都有其特定的应用场景：

- Service：直接用Service提供cluster内部的负载均衡，并借助cloud provider提供的LB提供外部访问
- Ingress Controller：还是用Service提供cluster内部的负载均衡，但是通过自定义LB提供外部访问
- Service Load Balancer：把load balancer直接跑在容器中，实现Bare Metal的Service Load Balancer
- Custom Load Balancer：自定义负载均衡，并替代kube-proxy，一般在物理部署Kubernetes时使用，方便接入公司已有的外部服务



# 资源对象

## Pods

*Pod* 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。

*Pod* 是一组（一个或多个） [容器](https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/#why-containers)； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器， 这些容器是相对紧密的耦合在一起的。 

除了应用容器，Pod 还可以包含在 Pod 启动期间运行的 [Init 容器](https://kubernetes.io/zh/docs/concepts/workloads/pods/init-containers/)。 你也可以在集群中支持[临时性容器](https://kubernetes.io/zh/docs/concepts/workloads/pods/ephemeral-containers/) 的情况下，为调试的目的注入临时性容器。

目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、 DaemonSet和PetSet。



### Pod 特征

- 包含多个共享IPC、Network和UTC namespace的容器，可直接通过localhost通信 
- 所有Pod内容器都可以访问共享的Volume，可以访问共享数据 
- Pod一旦调度后就跟Node绑定，即使Node挂掉也不会重新调度，推荐使用Deployments、Daemonsets等控制器来容错
- 优雅终止：Pod删除的时候先给其内的进程发送SIGTERM，等待一段时间（grace period）后才强制停止依然还在运行的进程 
- 特权容器（通过SecurityContext配置）具有改变系统配置的权限（在网络插件中大量应用）



### Pod 创建流程

在创建好 Deployment之后，Kubernetes 会根据这一定义创建符合要求的 Pod，并且通过在 Deployment 中定义的Label 筛选出对应的Pod实例并实时监控其状态和数量。如果实例数量少于定义的副本数量，则会根据在 Deployment 对象中定义的 Pod 模板创建一个新的Pod，然后将此 Pod 调度到合适的 Node 上启动运行，直到 Pod 实例的数量达到预定目标。 这个过程完全是自动化的，无须人工干预。 有了 deployment，服务扩容就变成一个纯粹的简单数字游戏了，只需要修改 Deployment 中的副本数量即可。后续的服务升级也将通过修改Deployment来自动完成。



### Pod 的生命周期



### 生命周期钩子

容器生命周期钩子（Container Lifecycle Hooks）监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数。支持两种钩子：

- postStart：容器启动后执行，注意由于是异步执行，它无法保证一定在 ENTRYPOINT之后运行。如果失败，容器会被杀死，并根据RestartPolicy决定是否重启
- preStop：容器停止前执行，常用于资源清理。如果失败，容器同样也会被杀死

而钩子的回调函数支持两种方式： 

- exec：在容器内执行命令
- httpGet：向指定URL发起GET请求

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: demo
spec:
	containers:
	- name: demo-container
	  image: nginx
	  lifecycle:
	  	postStart:
	  		exec:
	  			command: ["/bin/sh", "-c", "echo postStart"]
	  	preStop:
	  		exec:
	  			command: ["/usr/sbin/nginx", "-s", "quit"]
```



### Pod 类型

pod其实有两种类型：

- 普通 Pod：一旦被创建，就会放入 etcd 中存储，随后被 Kubernetes Master 调度到某个具体的 Node 上并绑定 (Binding)。该 pod 被对应的 Node 上的 kubelet 进程实例化成一组相关的 Docker 容器并启动。
- 静态 Pod: 它没有被存放在 Kubernetes 的 etcd中，而是被放在某个Node的一个具体文件中，并且只能在此Node上启动、运行。 

> 注意：在默认情况下，当Pod里的某个容器停止时，Kubernetes 会自动检测到这个问题并且重新启动这个Pod (重启 Pod 里的所有容器)，如果 Pod 所在的 Node 宕机，就会将这个 Node 上的所有 Pod 都重新调度到其他节点上。



### Pod 优先级

priority classes **优先级类**是一个 Kubernetes 对象，它允许我们将**数字优先级值**映射到特定的 Pod。那些具有更高价值的 Pod 被归类为更重要并且不太可能被驱逐。

可以通过命令查看当前优先级

```shell
$ kubectl get priorityclasses
NAME                      VALUE        GLOBAL-DEFAULT   AGE
system-cluster-critical   2000000000   false            83d
system-node-critical      2000001000   false            83d
```





### Init 容器

`Init Container`就是用来做初始化工作的容器，可以是一个或者多个，如果有多个的话，这些容器会按定义的顺序依次执行，只有所有的`Init Container`执行完后，主容器才会被启动。我们知道一个`Pod`里面的所有容器是共享数据卷和网络命名空间的，所以`Init Container`里面产生的数据可以被主容器使用到的。

Init Container在所有容器运行之前执行（run-to-completion），常用来初始化配置。 

```yaml
apiVersion: v1
kind: Pod
metadata: 
	name: init-demo
spec:
	containers:
	- name: nginx
	  image: nginx
	  ports: 
	  - containerPort: 80
	  volumeMounts:
	  - name: workdir
	    mountPath: /usr/share/nginx/html
	initContainers:
	- name: install
	  image: busybox
	  command: 
	  - wget
	  - "-O"
	  - "/work-dir/index.html"
	  - http://kubernetes.io
	  volumeMounts:
	  - name: workdir
	    mountPath: "/work-dir"
	dnsPolicy: Default
	volumes:
	- name: workdir
	  emptyDir: {}
```



### 临时容器

一种特殊的容器，该容器在现有 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 中临时运行，以便完成用户发起的操作，例如故障排查。 你会使用临时容器来检查服务，而不是用它来构建应用程序。

临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启， 因此不适用于构建应用程序。 临时容器使用与常规容器相同的 `ContainerSpec` 节来描述，但许多字段是不兼容和不允许的。

- 临时容器没有端口配置，因此像 `ports`，`livenessProbe`，`readinessProbe` 这样的字段是不允许的。
- Pod 资源分配是不可变的，因此 `resources` 配置是不允许的。
- 有关允许字段的完整列表，请参见 [EphemeralContainer 参考文档](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#ephemeralcontainer-v1-core)。

临时容器是使用 API 中的一种特殊的 `ephemeralcontainers` 处理器进行创建的， 而不是直接添加到 `pod.spec` 段，因此无法使用 `kubectl edit` 来添加一个临时容器。

与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。

**主要用途**

当由于容器崩溃或容器镜像不包含调试工具而导致 `kubectl exec` 无用时， 临时容器对于交互式故障排查很有用。

尤其是，[Distroless 镜像](https://github.com/GoogleContainerTools/distroless) 允许用户部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。 由于 distroless 镜像不包含 Shell 或任何的调试工具，因此很难单独使用 `kubectl exec` 命令进行故障排查。

使用临时容器时，启用 [进程名字空间共享](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/share-process-namespace/) 很有帮助，可以查看其他容器中的进程。



### 根容器

根容器 (Pause 容器)





### 私有镜像

在使用私有镜像时，需要创建一个`docker registry secret`，并在容器中引用。 

创建docker registry secret： 

```shell
kubectl create secret docker-registry regsecret --docker-server=<your -registry-server> --docker-username=<your-name> --docker-password=<yo ur-pword> --docker-email=<your-email>
```

容器中引用该 secret

```yaml
apiVersion: v1
kind: Pod
metadata: 
	name: private-reg 
spec: 
	containers: 
		- name: private-reg-container 
			image: <your-private-image> 
	imagePullSecrets: 
		- name: regsecret
```



### RestartPolicy

支持三种方式

- Always: 只要退出就重启
- OnFailure: 失败退出 (exit code 不等于 0) 时重启
- Never: 只要退出就不在重启

注意，这里的重启是指在Pod所在Node上面本地重启，并不会调度到其他Node上去。



### ImagePullPolicy

镜像拉取策略支持三种

- Always: 不管镜像是否存在都会进行一次拉取
- Never: 不管镜像是否存在都不会进行拉取
- IfNotPresent: 只有镜像不存在时，才会进行镜像拉取。

注意

- 默认为 IfNotPresent ，但 :latest 标签的镜像默认为 Always 。 
- 拉取镜像时docker会进行校验，如果镜像中的MD5码没有变，则不会拉取镜像数据。
- 生产环境中应该尽量避免使用 :latest 标签，而开发环境中可以借助 :latest 标签自动拉取最新的镜像。 



### ImagePullSecrets



### 访问DNS的策略

通过设置dnsPolicy参数，设置Pod中容器访问DNS的策略。

- ClusterFirst：优先基于cluster domain 后缀，通过kube-dns查询 
- Default：优先从kubelet中配置的DNS查询 

注意：

- 默认配置的dnsPolicy是ClusterFirst



### 健康检查

确保容器在部署后确实处在正常运行状态，Kubernetes提供了两种探针（Probe， 支持exec、tcp和httpGet方式）来探测容器的状态：

- LivenessProbe：探测应用是否处于健康状态，如果不健康则删除重建改容器 
- ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则更新容器的状态 

```yaml
apiVersion: v1
kind: Pod
metadata:
	labels:
		app: nginx
	name: nginx
spec:
	containers:
	- image: nginx
	  imagePullPolicy: Always
	  name: http
	  livenessProbe:
	  	httpGet:
	  	path: /
	  	port: 80
	  	initialDelaySeconds: 15
	  	timeoutSeconds: 1
	  readinessProbe:
	  	httpGet:
	  	path: /ping
	  	port: 80
	  	initialDelaySeconds: 5
	  	timeoutSeconds: 1
```



## Deployments

Deployment是一个定义及管理多副本应用（即多个副本 Pod）的新一代对象。

如果Pod出现故障，对应的服务也会挂掉，所以Kubernetes提供了一个Deployment的概念 ，目的是让Kubernetes去管理一组Pod的副本，也就是副本集 ，这样就能够保证一定数量的副本一直可用，不会因为某一个Pod挂掉导致整个服务挂掉。
Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。

这样使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 k8s 中，叫作"控制器"模式（controller pattern）。Deployment 扮演的正是 Pod 的控制器的角色。

部署是一个比RS应用模式更广的API对象， 可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。以K8s的发展方向，未来对所有长期伺服型的的业务的管理，都会通过Deployment来管理。 



例1：

```yaml
apiVersion: apps/v1 #注意版本号
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:  #属性，选择器
    matchLabels:
      app: nginx
  replicas: 2 #管理的副本个数
  template:	#模板属性
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        volumeMounts:  #定义挂载卷
        - mountPath: "/usr/share/nginx/html"
          name: nginx-vol
      volumes:   #定义共享卷
      - name: nginx-vol
        emptyDir: {}
```



[创建 Deployment 以将 ReplicaSet 上线](https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/#creating-a-deployment)。 ReplicaSet 在后台创建 Pods。 检查 ReplicaSet 的上线状态，查看其是否成功。

下面是一个 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 `nginx` Pods：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

注意，除了 Pod 的必填字段外，Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。 对于标签，请确保不要与其他控制器重叠。

`.spec.selector` 必须匹配 `.spec.template.metadata.labels`，否则请求会被 API 拒绝。



## ReplicaSet 副本集

RS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数使用。 



## Replication Controller  复制控制器

RC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中 运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就 会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。RC是K8s较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。 



## Service

RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访 问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止， 在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服 务。

在K8s集群中，客户端需要访问的服务就是Service对象。



##  DaemonSet 后台支撑服务集

长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的 Pod，有些节点上又没有这类Pod运行；而后台支撑型服务的核心关注点在K8s集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类Pod运行。

节点可能是所有集群节点也可能是通过nodeSelector选定的一些特定节点。典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支持K8s集群运行的服务。 



## StatefulSet 有状态服务集

> K8s在1.3版本里发布了Alpha版的PetSet功能。
>
> StatefulSet 之前曾用名 PetSet 名称

在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃 （disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、 不可丢弃（non-disposable）。

RC和RS主要是控制提供无状态服务的，其所控制的Pod 的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的 Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数。

而PetSet是用来控制有状态服务，PetSet中的每个Pod的名字都是事先确定的，不能更改。PetSet中 Pod的名字的作用是关联与该Pod对应的状态。

对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）；对于PetSet中 的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。 

有状态集群中一般会由如下特殊共性：

- 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以互相发现并通信
- 集群的规模是比较固定的，集群规模不能随意变动
- 集群中的每个节点都是有状态的，通常会持久化数据到永久存储中，每个节点在重启后都需要使用原有的持久化数据
- 集群中成员节点的启动顺序（以及关闭顺序）通常也是确定的
- 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损

> 如果通过 Deployment 控制 Pod 副本数来来实现有状态的集群，我们就会发现上述很多特性大部分难以满足，比如Deployment 创建的 Pod 因为 Pod 的名称是随机产生的，我们事先无法为每个 Pod 都确定唯一不变的ID，不同的Pod的启动顺序也无法保证，所以在集群中的某个成员节点宕机后，不能在其他节点上随意启动一个新的Pod实例。
>
> 为了能够在其他节点上恢复某个失败的节点，这种集群中的Pod需要挂接某种共享存储。

StatefulSet 从本质上说，可被看作 Deployment/RC 的一个特殊变种，有如下的特性：

- StatefulSet 里每个 Pod 都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设 StatefulSet 的名称为 kafka，那么第1个Pod叫 kafka-0，第 2 个叫 kafka-1，以此类推。
- StatefulSet 控制的Pod副本的启停顺序是受控的，操作第 n 个Pod时，前 n-1 个Pod已经是运行且准备好的状态。
- StatefulSet 里的Pod采用稳定的持久化存储卷，通过 PV 或 PVC 来实现，删除Pod时默认不会删除与 StatefulSet 相关的存储卷（为了保证数据安全）

StatefulSet 除了要与 Pv 卷捆绑使用，以存储 Pod 的状态数据，还要与 Headless Service 配合使用，即在每个 StatefulSet 定义中都要声明它属于哪个 Headless Service。StatefulSet 在 Headless Service 的基础上又为 StatefulSet 控制的每个 Pod 实例都创建了一个 DNS 域名，该域名格式如下：

```
$(podname).$(headless service name)
```

比如一个 3 节点的 Kafka 的 StatefulSet 集群对应的 Headless Service 的名称为 kafka，StatefulSet 的名称为 kafka，则StatefulSet里3个Pod的DNS名称分别为 kafka-0.kafka、kafka-1.kafka、kafka-2.kafka，这些DNS名称可以直接在集群的配置文件中固定下来。

> StatefulSet 的建模能力有限，面对复杂的有状态显的力不从心，所以就有了后来的 Kubernetes Operator 框架和众多 Operator 实现了。
>
> 需要注意的是，Kubernetes Operator 框架并不是面向普通用户的，而是面向开发者的。平台开发者借助 Operator 框架提供的API，可以方便的开发一个类似 StatefulSet的控制器，在这个控制器里通过编码的方式，实现对目标集群的自定义操控，包括集群部署、故障发现及集群调整等。
>
> 从发展趋势来看，未来主流的有状态集群基本都会以 Operator 方式部署到 Kubernetes 集群中。



## Federation 集群联邦



## ConfigMap

ConfigMap 顾名思义，就是保存配置项 (key=value) 的一个Map。ConfigMap 是分布式系统中 "配置中心"的独特实现之一。 几乎所有应用都需要一个静态的配置文件来提供启动参数，当这个应用是一个分布式应用，有多个副本部署在不同的机器上时，就可以使用 ConfigMap 来解决问题。

具体做法如下:

- 用户将配置文件的内容保存到 ConfigMap 中，文件名可作为 Key，value 就是整个文件的内容，多个配置文件都可被放入同一个 ConfigMap
- 在建模用户应用时，在 Pod 里将 ConfigMap 定义为特殊的 Volume 进行挂载。 在 Pod 被调度到某个具体 Node 上时，ConfigMap 里的配置文件会被自动还原到本地目录下，然后映射到 Pod 里指定的配置目录下，这样用户可以无感知的读取配置。
- 在 ConfigMap 的内容发生修改后，Kubernetes 会自动重新获取 ConfigMap 的内容，并在目标节点上更新对应的文件。

(补充拓扑图)



## Job 任务

Job是K8s用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别是：批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。

成功完成的标志根据不同的 spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。 

下列例子是计算圆周率 Job 配置

```yaml
apiVersion: batch/v1
kind: Job
metadata:
	name: pi
spec:
	template:
		spec:
			containers:
			- name: pi
				image: perl
				command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(1000)"]
			restartPolicy: Never
	parallelism: 1
	completions: 5
```

Jobs 控制器提供了两个控制并发数的参数：`completions` 和 `parallelism`，completions 表示需要运行任务数的总数，parallelism 表示并发运行的个数，例如设置 parallelism为1，则会依次运行任务，在前面的任务运行后再运行后面的任务。

Job所控制的 Pod 副本是短暂运行的，可以将其视为一组容器，其中的每个容器都仅运行一次。当 Job 控制的所有 Pod 副本都运行结束时，对应的 Job 也就结束了。

Job 在实现方式上与 Deployment 等副本控制器不同，Job生成的Pod副本是不能自动重启的。



## Cronjob 定时任务

类似 Linux Cron 的定时任务 Cron Job，它基本上照搬了 Linux Cron的表达式，格式如下

```
Minutes Hours DayofMonth Month DayofWeek
```

其中每个域都可出现的字符如下

- Minutes: `,` 、`-`、`*`、`/` 4个字符，有效范围为 0~59的整数。
- Hours:  `,` 、`-`、`*`、`/` 4个字符，有效范围为 0~23的整数。
- DayofMonth:  `,` 、`-`、`*`、`/` 、`?`、`L`、`W`、`C` 8个字符，有效范围为 1~31 的整数。
- Month:  `,` 、`-`、`*`、`/` 4个字符，有效范围为 1~12的整数或 JAN~DEC。
- DayofWeek: `,` 、`-`、`*`、`/` 、`?`、`L`、`W`、`#` 8个字符，有效范围为 1~7 的整数或 SUN~SAT。1表示星期天，2表示星期二，以此类推。

表达式中的特殊符号

- `*`: 表示匹配该域的任意值，假如在 Minutes 域使用 `*`，则表示每分钟都会触发事件。
- `/`: 表示从起始时间开始触发，然后每隔固定时间触发一次，例如在 Minutes 域设置为 5/20，则意味着第1次触发在第 5min 时，接下来每 20min 触发一次，将在第 25min、第45min 等时刻分别触发。



比如，要每隔 1min 执行一次任务，则 Cron 表达式如下

```
*/1 * * * *
```

Cron Job 的配置文件

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```









## 存储卷（Volume）

K8s集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一 个容器。而K8s的存储卷的生命周期和作用范围是一个Pod，当容器终止或重启时，Volume中的数据也不会丢失。每个Pod中声明的存储卷由Pod中的所有容器共享。

K8s支持非常多的存储卷类型，支持多种公有云平台的 存储，包括AWS，Google和Azure云；支持多种分布式存储包括GlusterFS和Ceph；也支持较容易使用的主机本地目录hostPath和NFS。

K8s还支持使用Persistent Volume Claim即PVC这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如AWS，Google或GlusterFS和Ceph），而将有关存储实际技术的配置交给存储管理员通过Persistent Volume来配置。

举例：在Pod上声明一个 Volume，然后在容器里引用该 Volume 并将其挂载(Mount) 到容器里的某个目录下

```yaml
apiVersion: v1
kind: Depoloyment
meta:
	name: tomcat
spec:
	template:
		metadata:
			labels:
				app: app-demo
				tier: frontend
		spec:
			volumes:
				- name: datavol
					emptyDir: {}
			containers:
			- name: tomcat-demo
				image: tomcat
				volumeMount: 
					- mountPath: /mydata
						name: datavol
				imagePullPolicy: IfNotPresent
```



### emptyDir

emptyDir 是在 Pod 分配到 Node 时创建的。从它的名称就可以看出，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为 Kubernetes 自动分配目录，当Pod从Node上移除时，emptyDir 中的数据也被永久移除。 

emptyDir 的一些用途如下：

- 临时空间，例如用于某些应用程序运行时所需的临时目录，且无需永久保留
- 长时间任务执行过程中使用的临时目录
- 一个容器需要从另一个容器中获取数据的目录 (多容器共享目录)

在默认情况下，emptyDir 使用的是节点的存储介质，例如磁盘或者网络存储。还可以使用 emptyDir.medium 属性，把这个属性设置为 "Memory"，就可以使用更快的基于内存的后端存储了。

> 注意：这种情况下的 emptyDir 使用的内存会被计入容器的内存消耗，将受到资源限制和配额机制的管理。



### hostPath

hostPath是在 Pod 上挂载宿主机上的文件或目录，通常可以用于如下用途：

- 在容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统对其进行存储
- 需要访问宿主机上 Docker 引擎内部数据结构的容器应用时，可以通过定义 hostPath 为宿主机 /var/lib/docker 目录，使容器内部的应用可以直接访问 Docker 的文件系统



在使用这种类型的 Volume，需要注意一下几点

- 在不同的Node上具有相同配置的 Pod，可能会因为宿主机上的目录和文件不同，而导致对 Volume 上目录和文件的访问结果不一致。
- 如果使用了资源配额管理，则 Kubernetes 无法将 hostPath 在宿主机上使用的资源纳入管理。

使用 hostPath类型定义 Volume

```yaml
volumes:
- name: "persistent-storage"
	hostPath:
		path: "/data"
```



### Persistent Volume 持久存储卷

PV 和 PVC  (持久存储卷声明 Persistent Volume Claim) 使得K8s集群具备了存储的逻辑抽象能力，使得在配置Pod的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。

存储的PV和PVC的这种关系，跟计算的Node和Pod的关系是非常类似的；PV和Node是 资源的提供者，根据集群的基础设施变化而变化，由K8s集群管理员配置；而PVC和 Pod是资源的使用者，根据业务服务的需求变化而变化，有K8s集群的使用者即服务的管理员来配置。

PV 表示由系统动态创建 (dynamically provisioned) 的一个存储卷，可以被理解成 Kubernetes 集群中某个网络存储对应的一块存储，它与 Volume 类似，但 PV 并不是被定义 Pod 上的，而是独立于 Pod 之外定义的。 PV目前支持的类型主要有 gcePersistentDisk、NFS、FC、ISCSI、CephFS、GlusterFS、HostPath、Local等



### StorageClass

> Kubernets 支持的存储系统有多种，如何知道从哪个存储系统中创建什么规格的 PV 存储卷呢？ 这就涉及 StorageClass 与 PVC，StorageClass 用来描述和定义某种存储系统的特征

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
	name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
	type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
	- debug
volumeBindingMode: Immediate
```

结合上面案例，StorageClass 有几个关键属性： provisioner、parameters 和 reclaimPolicy，系统在动态创建PV时会用到这几个参数。简单来说，provisioner代表了创建 PV 的第三方存储插件， parameters 是创建 PV 时的必要参数， reclaimPolicy 则表明了 PV 回收策略，回收策略包括删除或则保留。 

> 注意：StorageClass 的名称会在 PVC （PV Claim）中出现



### PVC 持久存储卷声明

典型的PVC定义

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: claim1
spec:
	accessModes:
		- ReadWriteOnce
	storageClassName: standard
	resources:
		requests:
			storage: 30Gi
```

PVC正如其名，表示应用希望申请的PV规格。其中重要的属性：accessModes(存储访问模式)、storageClassName(用哪种 StorageClass 来实现动态创建)、resources (存储的具体规格)

下面直接在Pod中引用 PVC即可

```yaml
spec:
	containers:
	- name: myapp
		image: tomcat
		volumeMounts:
			- name: tomcat-data
			  mountPath: "/data"
	volumes:
		- name: tomcat-data
		  persistentVolumeClaim:
		  	claimName: claim1
```



## Node

K8s集群中的计算能力由Node提供，最初Node称为服务节点Minion，后来改名为 Node。K8s集群中的Node也就等同于Mesos集群中的Slave节点，是所有Pod运行所在 的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统 一特征是上面要运行kubelet管理节点上运行的容器。 



## Secret 密钥对象

Secret是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用Secret的好处是可以避免把敏感信息明文写在配置文件里。在K8s集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问AWS存储的用户名密码。为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个 Secret对象，而在配置文件中通过Secret对象引用这些敏感信息。这种方式的好处包 括：意图明确，避免重复，减少暴漏机会。

> 在Pod中，是通过volume以及环境变量的方式去引用，确保了数据的安全性。
>
> 注意：ConfigMap 没有保密或加密的功能，存储加密数据常用Secret



secret类型

- **Service Account** 用于和API进行交互，会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录下
- **Opaque** 基于base64编码格式，用于加密以及解密
- **kubernetes.io/dockerconfigjson** 主要用于存储docker registry私用仓库的认证信息



### 账户

主要分为：用户账户 (User Account) 、服务账户 (Service Account)

顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和K8s集群中运行的 Pod提供账户标识。

用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人 的身份，人的身份与服务的namespace无关，所以用户账户是跨namespace的；而服务帐户对应的是一个运行中程序的身份，与特定namespace是相关的。 

Service Account 是通过 Secret 来保存对应的用户 (应用) 身份凭证的，这些凭证信息有 CA 根证书数据 (ca.crt) 和签名后的 Token 信息。



## HorizontalPodAutoscaler

可以将 HPA 理解为 Pod 横向自动扩容，即自动控制 Pod 数量的增加或减少。通过追踪分析制定 Deployment 控制的所有目标 Pod 的负载变化情况，来确定是否需要有针对性地调整目标 Pod 的副本数量，这是 HPA 的实现原理。

kubernetes 内置了基于 Pod 的CPU利用率进行自动扩缩容的机制，开发者可以自定义度量指标如每秒请求数，实现自定义的 HPA 功能。

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
	name: php-apache
	namespace: default
spec:
	maxReplicas: 10
	minReplicas: 1
	scaleTargetRef:
		kind: Deployment
		name: php-apache
	targetCPUUtilizationPercentage: 90
```

当CPU利用率的值超过 90%时，会触发动态扩容，限定 Pod 的副本数量为 1 ~ 10。



## Role

分别包含了两种角色类型：Role、ClusterRole

角色定义了一组特定权限的规则，比如可以操作某类资源对象。 局限于某个命名空间的角色由 Role 对象定义，作用于整个Kubernetes集群范围内的角色则通过 ClusterRole对象定义。

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
	namespace: default
	name: pod-reader
rules:
- apiGroups: [""] # 空字符串表明使用 core API Group
	resources: ["pods"]
	verbs: ["get", "watch", "list"]
```

表示在命名空间 default 中定义了一个 Role 对象，用于授予对 Pod 资源的读访问权限，绑定到该 Role 的用户则具有对 Pod 资源的 get、watch 和 list 权限。

接下来将Role 与具体用户绑定 (用户授权)

```yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
	name: read-pods
	namespace: default
subjects:
- kind: User
	name: Caden
	apiGroup: rbac.authorization.k8s.io
roleRef:
	kind: Role
	name: pod-reader
	apiGroup: rbac.authorization.k8s.io
```

> 表示在命名空间 default 中将 pod-reader 角色授予用户 Caden



## 快照、克隆



# 编排

## 常用编排yaml文件

### 限制网络带宽

可以通过给Pod增加 `kubernetes.io/ingress-bandwidth` 和 `kubernetes.io/egress-bandwidth` 这两个annotation来限制Pod的网络带宽

```yaml
aipVersion: v1
kind: Pod
metadata:
	name: qos
	annotations:
		kubernetes.io/ingress-bandwidth: 3M
		kubernetes.io/egress-bandwidth: 4M
spec:
	containers:
	- name: iperf3
	  image: networkstatic/iperf3
	  command: 
	  - iperf3
	  - -s
```



> kubenet的网络带宽限制其实是通过tc来实现的

```shell
$tc qdisc add dev cbr0 root handle 1: htb default 30
```



### 调度到指定的Node上

可以通过nodeSelector、nodeAffinity、podAffinity以及Taints和tolerations等来将Pod调度到需要的Node上。

也可以通过设置nodeName参数，将Pod调度到制定node节点上。 比如，使用nodeSelector，首先给Node加上标签:

```shell
$ kubectl label nodes xxx-node-name disktype=ssd
```

指定该Pod运行在带有 disktype=ssd 标签的Node上：

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: nginx
	labels:
		env: test
spec:
	containers:
	- name: nginx
	  image: nginx
	  imagePullPolicy: IfNotPresent
	nodeSelector:
	  disktype: ssd
```



### 自定义hosts

默认情况下，容器的 /etc/hosts 是kubelet自动生成的，并且仅包含localhost和podName等。不建议在容器内直接修改 /etc/hosts 文件，因为在Pod启动或重启时会被覆盖。

默认的 /etc/hosts 文件格式如下，其中 nginx-4217019353-fb2c5 是podName：

```shell
$ kubectl exec nginx-4217019353-fb2c5 -- cat /etc/hosts 
# Kubernetes-managed hosts file.
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback 
fe00::0 ip6-localnet 
fe00::0 ip6-mcastprefix 
fe00::1 ip6-allnodes 
fe00::2 ip6-allrouters 
10.244.1.4 nginx-4217019353-fb2c5
```

从v1.7开始，可以通过 `pod.Spec.HostAliases` 来增加hosts内容，如

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: hostaliases-pod
spec:
	hostAliases:
	- ip: "127.0.0.1"
	  hostname:
	  - "foo.local"
	  - "bar.local"
	- ip: "10.1.1.1"
	  hostname:
	  - "foo.remote"
	  - "bar.remote"
	containers:
	- name: cat-hosts
	  image: busybox
	  command:
	  - cat
	  args:
	  - "/etc/hosts"
```



















